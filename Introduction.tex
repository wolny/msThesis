
\chapter{Introduction}
\label{Introduction} 

\begin{quotation}
 It is impossible for any optimization algorithm to outperform random walks on all possible problems.
\end{quotation}
\begin{flushright}
 ... a conclusion from No Free Lunch Theorem
\end{flushright}

\section{A statement of a problem}

A Global Optimization Algorithm is defined as optimization algorithm
that employs measures that prevent convergence to local optima
and increase the probability of finding a global optimum. 

Evolutionary algorithms are known as a generic population-based metaheuristics
which often perform well approximating solutions to all
types of problems because they ideally do not make any assumption about the underlying fitness landscape; 
this generality is shown to be a great successes in many real-life problems. 
Evolutionary algorithms have the tendency to lose diversity within their
population of feasible solutions and to converge into a single solution.
However, there are domains where the global solution may not suffice. 
Such problems require the location and maintenance of multiple robust local
solutions, i.e local solutions whose basins of attraction areproperly wide and deep.

The most common technique in evolutionary algorithm which is
used to achieve this goal is to incorporate some sort of niching method
like crowding or fitness sharing which promote diversity of population,
which in turn delay premature convergence and likely enable the algorithm to
find multiple optimal solutions in single population.
 
Standard niching methods are often inefective and hard to introduce 
in existing evolutionary algorithms. In this paper we adopt a
different approach to multimodal function optimization. Instead of
embedding a niching method in the evolutionary algorithm itself we
use a hybrid approach in which we perform several runs of a evolutionary 
algorithm and alter the fitness function in every subsequent
run in a way that prevents exploration of basins of attraction which
were found in previous runs of the algorithm.

In each iteration we run EA, then cluster received population and
based on the assumption that clusters of individuals obtained from
the clustering algorithm are located in basins of attraction we interpolate
 each basin by multidimensional Gaussian function. By combining
these functions with current objective function in a proper way we
create deteriorated fitness function which will discourages future runs
from revisiting the same area.

This work tries to find an effective fitness deterioration technique in
high-dimensional domain spaces. We have implemented a general-purpose framework which can be
used to test our fitness deterioration techniques in conjunction with
various evolutionary algorithms. While our algorithm may be used with many types
of EAs it would be the most efficient when used with algorithms which are capable 
of finding many local solutions in single run. This is why for tests
we choose so called Hierarchical Genetic Strategy which performs efficient
concurrent search in the optimization landscape by many small populations.

The quality of the deterioration process strongly depends on clustering results.
We choose density-based algorithm called OPTICS as with this method we can extract clusters of
different densities very efficiently and choose clusters which give the
best accuracy of fitness deterioration process.

\section{Related Work}

As mentioned before this work focus on finding solutions for multi-modal
optimization tasks. There are many publications which decsribes how to extend
EAs to multi-modal optimization. Most of them focuses on 
\textit{Niching methods} \cite{niching,sharing,dynsharing} which address this
issue by maintaining a population of diverse solutions througout the time and this way they allow
parallel convergence into multiple good solutions in multimodal domains.

Our solution works by iterating a simple GA and maintaining the best solution of
each run off-line, by detection of basins of attraction and degeneration of
fitness landscape. We may consider our algorihtm as a variant of
\textit{Sequential niching} approach (throughout this paper we use terms 
\textit{Sequential niching} and \textit{Fitness deterioration} interchangeably).

At this point it is worth mentioning some of the works of Prof. A. Obuchowicz 
especially the publication \cite{esss} which is the only one I found which 
use the term \textit{fitness deterioration} explicitly.
In \cite{esss} he describes ESSS-DOF algorithm (Evolutionary Search with Soft
Selection with Deterioration of Objective Function) as an extension to the ESSS
method which maintaing population diversity by the following schema:
\begin{quotation}
When the population converges to local optimum we degenerate the objective 
function which cause the rapid migration of individuals and enable the 
population to escape for the local optimum.
\end{quotation}

The algorithm degenerate the objective function by composing it with Gaussian
function which approximate the local optimum. Our deterioration algorithm
described in detail in chapter 4 uses Gaussian functions as well
(Gaussian function has got many useful properties which makes it well-suited
to the fitness deterioration. We describe these characteristics in chapter 4).

Mentined methods are incorporated directly into the basic cycle of evolutionary
algorithm which differs from our \textit{Sequential niching} technique. 
The sequential niching approach has several advantages:

\begin{itemize}
  \item it is simple to incorporate in existing optimization methods
  \item it efficiently finds many local solutions
  \item it provides reasonable stop criterion which in this case is based on the
  quality of clusters returned by the clustering algorithm
\end{itemize}


